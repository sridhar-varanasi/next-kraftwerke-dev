# Starter pipeline
# Start with a minimal pipeline that you can customize to build and deploy your code.
# Add steps that build, run tests, deploy, and more:
# https://aka.ms/yaml

trigger:
- main

pool:
  vmImage: ubuntu-latest

variables:
- group: 'DEV'
- name: branchName
  value: $(Build.SourceBranch)
- name: folderName
  value: release

steps:
- task: PublishPipelineArtifact@1
  inputs:
    targetPath: '$(Build.Repository.LocalPath)/'
    artifact: 'Databricks'
    publishLocation: 'pipeline'
- script: pip install -U databricks-cli
  displayName: "install-databricks-cli"

- script: |
    echo "$(databricksHost)
    $(databricksToken)" | databricks configure --token
  displayName: 'configure databricks-cli'

- script: |
    databricks workspace ls
  displayName: 'test databricks cli'


- task: DownloadPipelineArtifact@2
  inputs:
    source: current
    artifact: 'Databricks'
    downloadPath: $(System.ArtifactsDirectory)/databricks

- script: 'ls $(System.ArtifactsDirectory)/databricks'

- script:
    
    FOLDER=$(echo /$(folderName))
    echo $folderName
    echo $FOLDER
    folder = $(databricks workspace ls --id $FOLDER)
    
    databricks workspace rm $folder --recursive


    echo $BRANCH_NAME
    

  displayName: 'Delete old release'

- script:
    BRANCH_NAME= $(echo  awk -F/ '($NF)')
    FOLDER=$(echo /$(Foldername))
    echo $FOLDER
    databricks import workspace import_dir $(System.ArtifactDirectory)/databricks.$FOLDER --exclude-hidden-files
  displayName: 'New Release'
