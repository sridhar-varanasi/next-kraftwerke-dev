# Starter pipeline
# Start with a minimal pipeline that you can customize to build and deploy your code.
# Add steps that build, run tests, deploy, and more:
# https://aka.ms/yaml

trigger:
- main

pool:
  vmImage: ubuntu-latest

variables:
- group: 'DEV'
- name: branchName
  value: $(Build.SourceBranch)
- name: folderName
  value: release

steps:
- task: PublishPipelineArtifact@1
  inputs:
    targetPath: '$(Build.Repository.LocalPath)/'
    artifact: 'Databricks'
    publishLocation: 'pipeline'
- script: pip install -U databricks-cli
  displayName: "install-databricks-cli"

- script: |
    echo "$(databricksHost)
    $(databricksToken)" | databricks configure --token
  displayName: 'configure databricks-cli'

- script: |
    databricks workspace ls
  displayName: 'test databricks cli'


- task: DownloadPipelineArtifact@2
  inputs:
    source: current
    artifact: 'Databricks'
    downloadPath: $(System.ArtifactsDirectory)/databricks

- script: 'ls $(System.ArtifactsDirectory)/databricks'

- script: |
    BRANCH_NAME=$(echo  awk -F/ '{print $NF}')
    FOLDER=$(echo '/$(folderName)')
    echo $FOLDER
    folder = $(databricks workspace ls --id $FOLDER)
    databricks workspace rm $FOLDER --recursive


  displayName: 'Delete old release'

- script: |
    BRANCH_NAME=$(echo  awk -F/ '{print $NF}')
    FOLDER=$(echo /$(folderName))
    echo $FOLDER
    databricks workspace import_dir $(System.ArtifactsDirectory)/databricks $FOLDER --exclude-hidden-files
  displayName: 'New Release'
